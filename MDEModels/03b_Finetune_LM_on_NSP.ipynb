{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cebebf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from transformers import BertConfig, BertForMaskedLM, BertPreTrainedModel, BertModel, PreTrainedTokenizerFast, DataCollatorForLanguageModeling, BertPreTrainedModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from packaging import version\n",
    "import datasets\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from utils.NSP_source_code import *\n",
    "from utils.computeMDE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c00a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compatability is tuned on Chopin only\n",
    "dataset_choice = 'Chopin43'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f6f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map = {\"Chopin43\": '_C', \n",
    "               \"ChopinAndHannds\": '_CH',\n",
    "               \"Maestro\": '_M'}\n",
    "Key = dataset_map[dataset_choice]\n",
    "handConfigNumsMap = {\"Chopin43\": 110, \n",
    "               \"ChopinAndHannds\": 136,\n",
    "               \"Maestro\": 12047}\n",
    "\n",
    "\n",
    "#There is only a compat dataset for Maestro currently\n",
    "MDEDir = './Extracted_Repns/MDE' + Key\n",
    "NSPDir = './Datasets/NSP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcbea0",
   "metadata": {},
   "source": [
    "#### Parse the Dataset's text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e615eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/mconati/ttmp/styletransfer/NSP/measures.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadf80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MDEDir +'/dict/handConf_dict', 'rb') as handle:\n",
    "    hands = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa0ae6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MEASURE', 'NEXT MEASURE', 'RANDOM MEASURE'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971ba06",
   "metadata": {},
   "source": [
    "# Modified huggingface code\n",
    "\n",
    "Here are the three elements that I modified from Huggingface. Modified elements are commented\n",
    "\n",
    "Modified BERT, the forward function is changed to calculate loss based only on NSP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed914050",
   "metadata": {},
   "source": [
    "### It wasn't 100% clear if we should finetune this task on MLM+NSP objectives or just NSP(or maybe MLM+NSP then NSP). The following code is the implementation to finetune on just NSP, but uncommenting the MLM loss calculation code will add that objective back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f60215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        self.decoder = {value:key for key, value in config.decoder.items()}\n",
    "        self.maskToken = config.decoder['[MASK]']\n",
    "        self.unkToken = config.decoder['[UNK]']\n",
    "        self.sepToken = config.decoder['[SEP]']\n",
    "        self.padToken = config.decoder['[PAD]']\n",
    "        self.clsToken = config.decoder['[CLS]']\n",
    "        self.specialTokens = [self.maskToken, self.unkToken, self.sepToken, self.padToken, self.clsToken]\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import BertTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        pitch_p, octave_p, hand_p, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        \n",
    "        #For calculating loss, decode the labels from MDE representation (ex 5s3s31) into three sequences\n",
    "        #of pitch, hand, octave\n",
    "#         if labels is not None:\n",
    "#             octaves = []\n",
    "#             pitches = []\n",
    "#             handConfs = []\n",
    "#             #Iterate through the batch\n",
    "#             for x in labels:\n",
    "#                 #For each sequence, make a list to store the octave, pitch, and handConf ids\n",
    "#                 octave = []\n",
    "#                 pitch = []\n",
    "#                 handConf = []\n",
    "#                 #Iterate through the sequence\n",
    "#                 for y in x:\n",
    "#                     #If the token is not a mask token, decode into the octave_pitch_handConf representation\n",
    "#                     if y.item() != -100 and y.item() not in self.specialTokens:\n",
    "#                         #Split on s\n",
    "#                         code = [int(x) for x in self.decoder[y.item()].split('s')]\n",
    "#                         #Add each element to the correct list\n",
    "#                         octave.append(code[0])\n",
    "#                         pitch.append(code[1])\n",
    "#                         handConf.append(code[2])\n",
    "#                     else:\n",
    "#                         #Otherwise, make a representation from the mask token ie, -100, -100, -100\n",
    "#                         octave.append(y.item())\n",
    "#                         pitch.append(y.item())\n",
    "#                         handConf.append(y.item())\n",
    "#                 #Aggregate the samples in the batch\n",
    "#                 octaves.append(octave)\n",
    "#                 pitches.append(pitch)\n",
    "#                 handConfs.append(handConf)\n",
    "            \n",
    "            \n",
    "#       Loss is cross entropy\n",
    "        loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            \n",
    "#             device = input_ids.device\n",
    "#             #Put the new labels on the gpu\n",
    "#             octaves = torch.LongTensor(octaves).to(device)\n",
    "#             pitches = torch.LongTensor(pitches).to(device)\n",
    "#             handConfs = torch.LongTensor(handConfs).to(device)\n",
    "            \n",
    "#             #Calculate a loss for each\n",
    "#             octave_loss = loss_fct(octave_p.view(-1, octave_p.shape[2]), octaves.view(-1))\n",
    "#             pitch_loss = loss_fct(pitch_p.view(-1, pitch_p.shape[2]), pitches.view(-1))\n",
    "#             hand_loss = loss_fct(hand_p.view(-1, hand_p.shape[2]), handConfs.view(-1))\n",
    "            \n",
    "#             #The returned loss is the sum of the three losses\n",
    "#             masked_lm_loss = octave_loss + pitch_loss + hand_loss\n",
    "\n",
    "        total_loss = 0# masked_lm_loss\n",
    "        if next_sentence_label is not None:\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = next_sentence_loss# + masked_lm_loss\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=[pitch_p, octave_p, hand_p],\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342322",
   "metadata": {},
   "source": [
    "### Custom encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1ddd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from octave, pitch, hand configuration, and position.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #Get the mapping from token to encoded representation\n",
    "        self.decoder = {value:key for key, value in config.decoder.items()}\n",
    "        \n",
    "        #Aggregate special tokens\n",
    "        self.maskToken = config.decoder['[MASK]']\n",
    "        self.unkToken = config.decoder['[UNK]']\n",
    "        self.sepToken = config.decoder['[SEP]']\n",
    "        self.padToken = config.decoder['[PAD]']\n",
    "        self.clsToken = config.decoder['[CLS]']\n",
    "        self.specialTokens = [self.maskToken, self.unkToken, self.sepToken, self.padToken, self.clsToken]\n",
    "        \n",
    "        #Declare embedding layers\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.pitch_embeddings = nn.Embedding(config.numPitches, config.hidden_size)\n",
    "        self.handConfig_embeddings = nn.Embedding(config.numConfigs, config.hidden_size)\n",
    "        self.octave_embeddings = nn.Embedding(config.numOctaves, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        #Custom code to use 3 embedding layers\n",
    "        #Convert the tokenized MDE representation ie 9s2s55 to pitch=9 octave=2 hand=55 for all tokens in the batch\n",
    "        octaves = []\n",
    "        pitches = []\n",
    "        handConfs = []\n",
    "        #Iterate through the batch\n",
    "        for x in input_ids:\n",
    "            #For each sequence, make a list to store the octave, pitch, and handConf ids\n",
    "            octave = []\n",
    "            pitch = []\n",
    "            handConf = []\n",
    "            #Iterate through the sequence\n",
    "            for y in x:\n",
    "                #If the token is not a special token, decode into the octave_pitch_handConf representation\n",
    "                if y.item() not in self.specialTokens:\n",
    "                    #Split on s\n",
    "                    try:\n",
    "                        code = [int(x) for x in self.decoder[y.item()].split('s')]\n",
    "                    except:\n",
    "                        code = [x for x in self.decoder[y.item()].split('s')]\n",
    "                        print(code)\n",
    "                    #Add each element to the correct list\n",
    "                    octave.append(code[0])\n",
    "                    pitch.append(code[1])\n",
    "                    handConf.append(code[2])\n",
    "                else:\n",
    "                    #Otherwise, make a representation from the special token. ie: a cls token(1) becomes 1_1_1\n",
    "                    octave.append(y.item())\n",
    "                    pitch.append(y.item())\n",
    "                    handConf.append(y.item())\n",
    "            #Aggregate the samples in the batch\n",
    "            octaves.append(octave)\n",
    "            pitches.append(pitch)\n",
    "            handConfs.append(handConf)\n",
    "            \n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        \n",
    "        #Convert the lists to tensors and put them on the gpu\n",
    "        octTensor = torch.LongTensor(octaves).to(device)\n",
    "        pitchTensor = torch.LongTensor(pitches).to(device)\n",
    "        handConfTensor = torch.LongTensor(handConfs).to(device)\n",
    "        \n",
    "        #Sum the three embeddings\n",
    "        input_embeds = self.handConfig_embeddings(handConfTensor)\\\n",
    "                       +self.octave_embeddings(octTensor)\\\n",
    "                       +self.pitch_embeddings(pitchTensor)\n",
    "        embeddings = input_embeds\n",
    "\n",
    "        #Standard BertEmbeddings code\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531c633",
   "metadata": {},
   "source": [
    "### Custom Configuration\n",
    "\n",
    "In order for the encoder and MaskedLM to access the dictionary between MDE representation and tokens, we need to pass that in the model's config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d4a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertConfig(BertConfig):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #The decoder holds the conversion back to the coded representation for the customEmbeddings layer\n",
    "        self.decoder = kwargs.get('decoder')\n",
    "        self.numOctaves = 9\n",
    "        self.numConfigs = handConfigNumsMap[dataset_choice]\n",
    "        self.numPitches = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5f7a8",
   "metadata": {},
   "source": [
    "Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b724a5a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmconati\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "#1dd35d404a289e1e49f18069e4fe0a51d28d52c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tokenizer\n",
    "TOKENIZER_SAVEDIR = Path(MDEDir + '/tokenizer')\n",
    "LM_MODEL_SAVEDIR = Path(MDEDir + '/model/NSP')\n",
    "Path(LM_MODEL_SAVEDIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc29fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46226d",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eeef641",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='Extracted_Repns/MDE_C/tokenizer', vocab_size=1605, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_SAVEDIR)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a68bfa",
   "metadata": {},
   "source": [
    "Create the Sentence pairings, and save them as a file to be read by the dataset creation code. This can be commented out if ian inputs.pickle exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7cb9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for idx, measure in enumerate(df['MEASURE']):\n",
    "    \n",
    "    #If they are consecutive add a 0 to the labels\n",
    "    sentence_a.append(computeMDE(NSPDir + '/' + df['MEASURE'][idx], hands))\n",
    "    sentence_b.append(computeMDE(NSPDir + '/' + df['NEXT MEASURE'][idx], hands))\n",
    "    label.append(0)\n",
    "    \n",
    "    #If not add a 1\n",
    "    sentence_a.append(computeMDE(NSPDir + '/' + df['MEASURE'][idx], hands))\n",
    "    sentence_b.append(computeMDE(NSPDir + '/' + df['RANDOM MEASURE'][idx], hands))\n",
    "    label.append(1)\n",
    "\n",
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=64, truncation=True, padding='max_length')\n",
    "inputs['labels'] = torch.LongTensor([label]).T\n",
    "with open (NSPDir + '/inputs.pickle', 'wb') as handle:\n",
    "    pickle.dump(inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9290a",
   "metadata": {},
   "source": [
    "Validate that inputs look as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d96ec067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  1, 267, 312,  ...,   3,   3,   3],\n",
       "        [  1, 267, 312,  ...,   3,   3,   3],\n",
       "        [  1, 501, 547,  ...,   3,   3,   3],\n",
       "        ...,\n",
       "        [  1,  50,  29,  ...,   3,   3,   3],\n",
       "        [  1,  44,   0,  ...,   3,   3,   3],\n",
       "        [  1,  44,   0,  ...,   3,   3,   3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ...,\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(NSPDir + '/inputs.pickle', 'rb') as handle:\n",
    "    inputs = pickle.load(handle)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2794e1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ca26d",
   "metadata": {},
   "source": [
    "Load the inputs pickle into a dataset form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36fb3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        returnee = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        returnee['next_sentence_label'] = returnee.pop('labels')\n",
    "        return returnee\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22bd83ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  1, 267, 312, 267, 312,   2, 267, 312, 267, 312,   2,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'next_sentence_label': tensor([0])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NSPDataset(inputs)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0a2cb",
   "metadata": {},
   "source": [
    "Using a data collator for language modeling, but the MLM objective is disabled in the forward function. This can be easily reenabled by changing the forward function and mlm_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d35903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT training code basically copied from the Huggingface Esperanto Tutorial from here on out\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97806405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1605"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CustomBertConfig(\n",
    "    #The decoder holds the conversion back to the coded representation for the customEmbeddings layer\n",
    "    decoder = tokenizer.vocab,\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    ")\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4004d2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 87968389\n",
      "Num parameters: 86834821\n"
     ]
    }
   ],
   "source": [
    "#Create a standard BERT model\n",
    "model = BertForPreTraining(config=config)\n",
    "print('Num parameters:', model.num_parameters())\n",
    "\n",
    "\n",
    "#Create a custom embedding class\n",
    "temp = CustomBertEmbeddings(config)\n",
    "#Replace the model's embedding layer\n",
    "model.bert.embeddings = temp\n",
    "\n",
    "\n",
    "#As a sanity check, make sure that the custom embedding layers exist\n",
    "model.bert.embeddings.handConfig_embeddings.weight\n",
    "print('Num parameters:', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "982d0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8510482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=10,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc77d142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=5,\n",
       "evaluation_strategy=IntervalStrategy.STEPS,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=Extracted_Repns/MDE_C/model/NSP/runs/May09_23-05-17_mirlab6,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=5,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=100,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "output_dir=Extracted_Repns/MDE_C/model/NSP,\n",
       "overwrite_output_dir=True,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=128,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['wandb'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=Extracted_Repns/MDE_C/model/NSP,\n",
       "save_on_each_node=False,\n",
       "save_steps=10,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=1,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "trainer.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e0efaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1612\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 700\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mconati/StyleTransferClean/wandb/run-20220509_230522-10uki4ju</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mconati/huggingface/runs/10uki4ju\" target=\"_blank\">Extracted_Repns/MDE_C/model/NSP</a></strong> to <a href=\"https://wandb.ai/mconati/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 23:43, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.096200</td>\n",
       "      <td>0.689439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.767400</td>\n",
       "      <td>0.690972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.698600</td>\n",
       "      <td>0.693431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.690141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>0.689278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.682641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.677650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.666909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.667661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.650907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.642136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.654200</td>\n",
       "      <td>0.684966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.699800</td>\n",
       "      <td>0.690017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.638906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.639800</td>\n",
       "      <td>0.631824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.644300</td>\n",
       "      <td>0.618326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.596004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.604821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.582782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.544455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.551700</td>\n",
       "      <td>0.518919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.546400</td>\n",
       "      <td>0.539505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.534200</td>\n",
       "      <td>0.479449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.518300</td>\n",
       "      <td>0.482246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>0.462107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.415797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.429189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.395072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.444300</td>\n",
       "      <td>0.390679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.387507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.372318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.331056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.382339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.313761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.310109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.284676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.273362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.254349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.274232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.351700</td>\n",
       "      <td>0.299380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.232495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.226377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.276492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.198536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.238800</td>\n",
       "      <td>0.187440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.234530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.177322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.162988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.192900</td>\n",
       "      <td>0.178519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.165806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.217405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.156105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.163755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.170518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.160573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.134902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.132462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.126623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.149529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.122316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.114959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.106193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.111864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.152700</td>\n",
       "      <td>0.105835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.117468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.096792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.095896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.099419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.122530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.092194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>0.106659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.089383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.116882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.087614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>0.074617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.106401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.070177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.065366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.073192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.058346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.061628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.092930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.063491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.093635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.074018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.093466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.051203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.051649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.052964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.056216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.053340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.048988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.050193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.061327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.047684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.056159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.049890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.045984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.047496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.036638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.042777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.050071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.035808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.040154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.044234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.031464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.043153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.031205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.033937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.030238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.058952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.030269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.044967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.028554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.052519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.033422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.027631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.050863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.026881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.026620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.029361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.025821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.025534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.028103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.025503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.025204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.029320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.035189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.028539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.025651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.024690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.025177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.025475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.026548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.029199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.028183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.026482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.025624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.025208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-10\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-10/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-50] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-20\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-20/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-10] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-30\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-30/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-20] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-40\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-40/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-30] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-50\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-50/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-40] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-60\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-60/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-60/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-70\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-70/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-60] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-80\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-80/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-70] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-90\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-90/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-80] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-100\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-100/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-90] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-110\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-110/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-110/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-100] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-120\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-120/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-110] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-130\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-130/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-130/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-120] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-140\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-140/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-140/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-130] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-150\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-150/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-140] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-160\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-160/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-150] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-170\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-170/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-170/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-160] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-180\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-180/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-180/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-170] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-190\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-190/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-190/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-180] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-200\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-200/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-190] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-210\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-210/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-210/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-200] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-220\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-220/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-220/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-230\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-230/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-230/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-210] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-220] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-240\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-240/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-230] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-250\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-250/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-250/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-260\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-260/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-260/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-250] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-270\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-270/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-270/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-240] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-260] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-280\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-280/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-280/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-270] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-290\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-290/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-290/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-280] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-300\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-300/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-300/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-310\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-310/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-310/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-290] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-300] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-320\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-320/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-310] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-330\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-330/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-330/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-340\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-340/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-340/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-320] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-330] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-350\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-350/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-350/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-360\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-360/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-360/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-350] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-370\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-370/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-370/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-360] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-380\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-380/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-380/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-340] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-370] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-390\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-390/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-390/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-380] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-400\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-400/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-400/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-410\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-410/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-410/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-390] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-400] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-420\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-420/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-420/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-430\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-430/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-430/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-420] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-440\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-440/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-440/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-410] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-430] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-450\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-450/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-450/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-460\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-460/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-460/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-450] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-470\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-470/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-470/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-440] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-460] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-480\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-480/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-470] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-490\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-490/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-490/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-500\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-500/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-490] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-510\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-510/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-510/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-500] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-520\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-520/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-520/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-510] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-530\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-530/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-530/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-540\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-540/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-540/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-530] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-550\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-550/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-550/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-520] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-540] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-560\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-560/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-560/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-570\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-570/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-570/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-560] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-580\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-580/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-580/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-570] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-590\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-590/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-590/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-550] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-580] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-600\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-600/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-590] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-610\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-610/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-610/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-620\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-620/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-620/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-600] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-610] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-630\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-630/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-630/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-620] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-640\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-640/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-640/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-650\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-650/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-650/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-640] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-660\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-660/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-660/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-630] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-650] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-670\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-670/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-670/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-680\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-680/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-680/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-670] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-690\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-690/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-690/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-680] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_C/model/NSP/checkpoint-700\n",
      "Configuration saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-700/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_C/model/NSP/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_C/model/NSP/checkpoint-690] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from Extracted_Repns/MDE_C/model/NSP/checkpoint-660 (score: 0.024689989164471626).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.22964170221771513, metrics={'train_runtime': 1434.6605, 'train_samples_per_second': 112.361, 'train_steps_per_second': 0.488, 'total_flos': 5344576787097600.0, 'train_loss': 0.22964170221771513, 'epoch': 100.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c1a59",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# I'm doubtful of how low the validation loss goes on this training. It needs further investigation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
