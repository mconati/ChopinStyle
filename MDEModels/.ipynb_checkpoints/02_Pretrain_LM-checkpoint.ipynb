{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cebebf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from transformers import BertConfig, BertForMaskedLM, BertModel, PreTrainedTokenizerFast, DataCollatorForLanguageModeling, BertPreTrainedModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "import datasets\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from utils.LM_source_code import *\n",
    "from utils.tokenizer_utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88404e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHOOSE THE DATASET\n",
    "\n",
    "#All options are: 1. Chopin43  \n",
    "                # 2. Maestro\n",
    "                # 3. Chopin and Hannds\n",
    "        \n",
    "        \n",
    "dataset_choice = 'Maestro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c00a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map = {\"Chopin43\": '_C', \n",
    "               \"ChopinAndHannds\": '_CH',\n",
    "               \"Maestro\": '_M'}\n",
    "Key = dataset_map[dataset_choice]\n",
    "MDEDir = './Extracted_Repns/MDE' + Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab179be",
   "metadata": {},
   "source": [
    "# Setting up MDE BERT\n",
    "The MDE representation consists of 3 things for every note onset in a piece:\n",
    "1. The pitch of the lowest note (a number 1-12 corresponding to A, A#, ....)\n",
    "2. The octave of the lowest note (a number 0-7 corresponding to which octave on the piano)\n",
    "3. The hand configuration (a number 0-136 that maps to a of hand configurations)\n",
    "\n",
    "To feed this information into BERT with standard huggingface components, I am using an intermediate, proxy tokenizer. At each step, the three elements are concatenated with a separation character. For example, a step with pitch 6, octave 2, and hand configuration 12 becomes 6s2s12. With this intermediate step, the sequence is dimension (1, max_seq_len) instead of (3, max_seq_len), so the default huggingface tokenizers, collators, and trainers can be used.\n",
    "\n",
    "Within the modified components, there are 3 separate embedding layers at the encoder and decoder(we effectively have three separate vocabularies). The first step in the encoder is to convert the MDE representation(ie. 6s2s12) back to a separate pitch, octave, hand representation so that it can be passed throgh three separate embedding layers.The outputs of the embedding layers can then be summed so that input to the attention layers has the normal shape (768). The decoder then has to have three separate linear layers to map the hidden state back to pitch, octave, hand.\n",
    "\n",
    "In order to make these changes in huggingface, we need to construct a custom encoder by modifying the BertEmbedding layer, and a custom decoder by modifying the BertLMPredictionHead. We also need to modify the BertForMaskedLM model itself so that the forward function expects three outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971ba06",
   "metadata": {},
   "source": [
    "# Modified huggingface code\n",
    "\n",
    "Here are the three elements that I modified from Huggingface. Modified elements are commented\n",
    "\n",
    "Modified MaskedLM, the forward function is changed to calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11f9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMaskedLM(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.decoder = {value:key for key, value in config.decoder.items()}\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
    "            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        #The new decoder returns pitch, octave, and hand predictions\n",
    "        pitch_p, octave_p, hand_p = self.cls(sequence_output)\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        \n",
    "        #For calculating loss, decode the labels from MDE representation (ex 5s3s31) into three sequences\n",
    "        #of pitch, hand, octave\n",
    "        if labels is not None:\n",
    "            octaves = []\n",
    "            pitches = []\n",
    "            handConfs = []\n",
    "            #Iterate through the batch\n",
    "            for x in labels:\n",
    "                #For each sequence, make a list to store the octave, pitch, and handConf ids\n",
    "                octave = []\n",
    "                pitch = []\n",
    "                handConf = []\n",
    "                #Iterate through the sequence\n",
    "                for y in x:\n",
    "                    #If the token is not a mask token, decode into the octave_pitch_handConf representation\n",
    "                    if y.item() != -100:\n",
    "                        #Split on s\n",
    "                        code = [int(x) for x in self.decoder[y.item()].split('s')]\n",
    "                        #Add each element to the correct list\n",
    "                        octave.append(code[0])\n",
    "                        pitch.append(code[1])\n",
    "                        handConf.append(code[2])\n",
    "                    else:\n",
    "                        #Otherwise, make a representation from the mask token ie, -100, -100, -100\n",
    "                        octave.append(y.item())\n",
    "                        pitch.append(y.item())\n",
    "                        handConf.append(y.item())\n",
    "                #Aggregate the samples in the batch\n",
    "                octaves.append(octave)\n",
    "                pitches.append(pitch)\n",
    "                handConfs.append(handConf)\n",
    "            \n",
    "            \n",
    "            #Loss is still cross entropy\n",
    "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            \n",
    "            device = input_ids.device\n",
    "            #Put the new labels on the gpu\n",
    "            octaves = torch.LongTensor(octaves).to(device)\n",
    "            pitches = torch.LongTensor(pitches).to(device)\n",
    "            handConfs = torch.LongTensor(handConfs).to(device)\n",
    "            \n",
    "            #Calculate a loss for each\n",
    "            octave_loss = loss_fct(octave_p.view(-1, octave_p.shape[2]), octaves.view(-1))\n",
    "            pitch_loss = loss_fct(pitch_p.view(-1, pitch_p.shape[2]), pitches.view(-1))\n",
    "            hand_loss = loss_fct(hand_p.view(-1, hand_p.shape[2]), handConfs.view(-1))\n",
    "            \n",
    "            #The returned loss is the sum of the three losses\n",
    "            masked_lm_loss = octave_loss + pitch_loss + hand_loss\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "        \n",
    "        #This is where loss and predictions are returned\n",
    "        #TODOs: The current model can't use the pipeline feature of transformers\n",
    "        #since the pipeline expects logits to just be a tokenized MDE representation\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=[pitch_p, octave_p, hand_p],\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    " \n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        effective_batch_size = input_shape[0]\n",
    "\n",
    "        #  add a dummy token\n",
    "        if self.config.pad_token_id is None:\n",
    "            raise ValueError(\"The PAD token should be defined for generation\")\n",
    "\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
    "        dummy_token = torch.full(\n",
    "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01541bf2",
   "metadata": {},
   "source": [
    "### Custom decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4342c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        \n",
    "        #Three separate linear layers for converting hidden state to outputs\n",
    "        self.decode_pitch = nn.Linear(config.hidden_size, config.numPitches, bias=False)\n",
    "        self.decode_octave = nn.Linear(config.hidden_size, config.numOctaves, bias=False)\n",
    "        self.decode_hand = nn.Linear(config.hidden_size, config.numConfigs, bias=False)\n",
    "        \n",
    "        self.bias_p = nn.Parameter(torch.zeros(config.numPitches))\n",
    "        self.bias_o = nn.Parameter(torch.zeros(config.numOctaves))\n",
    "        self.bias_h = nn.Parameter(torch.zeros(config.numConfigs))\n",
    "\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decode_pitch.bias = self.bias_p\n",
    "        self.decode_octave.bias = self.bias_o\n",
    "        self.decode_hand.bias = self.bias_h\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        #Pass the hidden state through the three decoder layers\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        pitch = self.decode_pitch(hidden_states)\n",
    "        octave = self.decode_octave(hidden_states)\n",
    "        hand = self.decode_hand(hidden_states)\n",
    "        return pitch, octave, hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342322",
   "metadata": {},
   "source": [
    "### Custom encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ddd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from octave, pitch, hand configuration, and position.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #Get the mapping from token to encoded representation\n",
    "        self.decoder = {value:key for key, value in config.decoder.items()}\n",
    "        \n",
    "        #Aggregate special tokens\n",
    "        self.maskToken = config.decoder['[MASK]']\n",
    "        self.unkToken = config.decoder['[UNK]']\n",
    "        self.sepToken = config.decoder['[SEP]']\n",
    "        self.padToken = config.decoder['[PAD]']\n",
    "        self.clsToken = config.decoder['[CLS]']\n",
    "        self.specialTokens = [self.maskToken, self.unkToken, self.sepToken, self.padToken, self.clsToken]\n",
    "        \n",
    "        #Declare embedding layers\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.pitch_embeddings = nn.Embedding(config.numPitches, config.hidden_size)\n",
    "        self.handConfig_embeddings = nn.Embedding(config.numConfigs, config.hidden_size)\n",
    "        self.octave_embeddings = nn.Embedding(config.numOctaves, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        #Custom code to use 3 embedding layers\n",
    "        #Convert the tokenized MDE representation ie 9s2s55 to pitch=9 octave=2 hand=55 for all tokens in the batch\n",
    "        octaves = []\n",
    "        pitches = []\n",
    "        handConfs = []\n",
    "        #Iterate through the batch\n",
    "        for x in input_ids:\n",
    "            #For each sequence, make a list to store the octave, pitch, and handConf ids\n",
    "            octave = []\n",
    "            pitch = []\n",
    "            handConf = []\n",
    "            #Iterate through the sequence\n",
    "            for y in x:\n",
    "                #If the token is not a special token, decode into the octave_pitch_handConf representation\n",
    "                if y.item() not in self.specialTokens:\n",
    "                    #Split on s\n",
    "                    try:\n",
    "                        code = [int(x) for x in self.decoder[y.item()].split('s')]\n",
    "                    except:\n",
    "                        code = [x for x in self.decoder[y.item()].split('s')]\n",
    "                        print(code)\n",
    "                    #Add each element to the correct list\n",
    "                    octave.append(code[0])\n",
    "                    pitch.append(code[1])\n",
    "                    handConf.append(code[2])\n",
    "                else:\n",
    "                    #Otherwise, make a representation from the special token. ie: a cls token(1) becomes 1_1_1\n",
    "                    octave.append(y.item())\n",
    "                    pitch.append(y.item())\n",
    "                    handConf.append(y.item())\n",
    "            #Aggregate the samples in the batch\n",
    "            octaves.append(octave)\n",
    "            pitches.append(pitch)\n",
    "            handConfs.append(handConf)\n",
    "            \n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        \n",
    "        #Convert the lists to tensors and put them on the gpu\n",
    "        octTensor = torch.LongTensor(octaves).to(device)\n",
    "        pitchTensor = torch.LongTensor(pitches).to(device)\n",
    "        handConfTensor = torch.LongTensor(handConfs).to(device)\n",
    "        \n",
    "        #Sum the three embeddings\n",
    "        input_embeds = self.handConfig_embeddings(handConfTensor)\\\n",
    "                       +self.octave_embeddings(octTensor)\\\n",
    "                       +self.pitch_embeddings(pitchTensor)\n",
    "        embeddings = input_embeds\n",
    "\n",
    "        #Standard BertEmbeddings code\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531c633",
   "metadata": {},
   "source": [
    "### Custom Configuration\n",
    "\n",
    "In order for the encoder and MaskedLM to access the dictionary between MDE representation and tokens, we need to pass that in the model's config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72855cd",
   "metadata": {},
   "source": [
    "# THIS MUST BE CHANGED FOR DIFFERENT DATASETS. THE numCONFIGS COMES FROM CREATING THE HANDCONIG DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d4a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertConfig(BertConfig):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #The decoder holds the conversion back to the coded representation for the customEmbeddings layer\n",
    "        self.decoder = kwargs.get('decoder')\n",
    "        self.numOctaves = 9\n",
    "        #110 for Chopin43\n",
    "        #720 for Maestro\n",
    "        #136 for ChopinAndHannds\n",
    "        self.numConfigs = 12047\n",
    "        self.numPitches = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5f7a8",
   "metadata": {},
   "source": [
    "# Setup the Tokenizer and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b724a5a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmconati\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#login to wandb for logging\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup paths\n",
    "TOKENIZER_SAVEDIR = Path(MDEDir + '/tokenizer')\n",
    "LM_MODEL_SAVEDIR = Path(MDEDir + '/model')\n",
    "Path(LM_MODEL_SAVEDIR).mkdir(exist_ok=True)\n",
    "\n",
    "PRIMUS_TXT_FILES = Path(MDEDir + '/text')\n",
    "TXT_LOCATION = Path(MDEDir + '/full')\n",
    "files = [str(TXT_LOCATION / path) for path in os.listdir(TXT_LOCATION)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d546d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7f3e40db0c50>, <torch.cuda.device at 0x7f3e40db0fd0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify a GPU is enabled\n",
    "\n",
    "print('Cuda available: ', torch.cuda.is_available())\n",
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0eb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Create a vocab for the dictionary of tokens that appear at least (threshold) times\n",
    "def makeVocab(MDEDir, threshold = 2):\n",
    "    tpath = MDEDir + '/full/MDE.txt'\n",
    "    my_file = open(tpath, \"r\")\n",
    "\n",
    "    # reading the file\n",
    "    data = my_file.read()\n",
    "\n",
    "    # replacing end of line('/n') with ' ' and\n",
    "    # splitting the text it further when '.' is seen.\n",
    "    data = np.array(data.replace('\\n', '').split(\" \"))[:-1]\n",
    "    print(data)\n",
    "\n",
    "    u,counts = np.unique(data, return_counts=True)\n",
    "    pairs = sorted(zip(counts,u), reverse=True)\n",
    "    counts, u = zip(*pairs)\n",
    "    \n",
    "    plt.plot(sorted(counts, reverse=True))\n",
    "    plt.title(\"Vocab\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Token number\")\n",
    "    \n",
    "    threshold = threshold\n",
    "    for idx, c in enumerate(counts):\n",
    "        if c < threshold:\n",
    "            break\n",
    "    vocab_size = idx\n",
    "\n",
    "    vocab = {}\n",
    "    vocab['[UNK]'] = 0\n",
    "    vocab['[CLS]'] = 1\n",
    "    vocab['[SEP]'] = 2\n",
    "    vocab['[PAD]'] = 3\n",
    "    vocab['[MASK]'] = 4\n",
    "    token_offset = 5\n",
    "    for i,qanon in enumerate(u):\n",
    "        if i > vocab_size:\n",
    "            break\n",
    "        vocab[qanon] = token_offset + i \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eeef641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5s2s1' '5s5s1' '5s10s1' ... '4s6s1' '3s6s1' '2s6s1']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create the vocab and make a custom tokenizer from it\n",
    "vocab = makeVocab(MDEDir, 2)\n",
    "tokenizer = BertTokenizer(vocab)\n",
    "\n",
    "#Save the tokenizer\n",
    "tokenizer_dir = TOKENIZER_SAVEDIR\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "with open(f'{tokenizer_dir}/vocab.pkl','wb') as f:\n",
    "    pickle.dump(vocab,f)\n",
    "\n",
    "#Visualization of the vocab. Many tokens are very uncommon, the vocab threshold\n",
    "#can be adjusted as desired to remove uncommon tokens from the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335699d1",
   "metadata": {},
   "source": [
    "# Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97806405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28013"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CustomBertConfig(\n",
    "    #The decoder holds the conversion back to the coded representation for the customEmbeddings layer\n",
    "    decoder = tokenizer.vocab,\n",
    "    vocab_size=len(vocab),\n",
    ")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4004d2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Num parameters: 107584877\n",
      "New Num parameters: 104589860\n",
      "\n",
      "Shape of the hand configurations embedding layer:\n",
      "torch.Size([12047, 768])\n"
     ]
    }
   ],
   "source": [
    "#Create a standard BERT model\n",
    "model = BertForMaskedLM(config=config)\n",
    "device = model.device\n",
    "print('Original Num parameters:', model.num_parameters())\n",
    "\n",
    "\n",
    "#Create a custom embedding class\n",
    "temp = CustomBertEmbeddings(config).to(device)\n",
    "#Replace the model's embedding layer\n",
    "model.bert.embeddings = temp\n",
    "\n",
    "#create a custom decoder\n",
    "temp = CustomBertLMPredictionHead(config).to(device)\n",
    "#replace the custom decoder\n",
    "model.cls.predictions = temp\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "print('New Num parameters:', model.num_parameters())\n",
    "print()\n",
    "\n",
    "#As a sanity check, make sure that the custom embedding layers exist\n",
    "print(\"Shape of the hand configurations embedding layer:\")\n",
    "print(model.bert.embeddings.handConfig_embeddings.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b376101",
   "metadata": {},
   "source": [
    "# Setup the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf3b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of the training\n",
    "\n",
    "MAX_LEN = 128 #max length of training sequences\n",
    "MASKING_PROPORTION = 0.15 #masking proportion for MLM training\n",
    "NUM_EPOCHS = 2000\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f886ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Custom dataset for the MLM task. Creates sequences of length max_length from the text files\n",
    "#, and pads as needed\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_files, tokenizer, max_length=64):\n",
    "        self.examples = []\n",
    "        for src_file in tqdm.tqdm(src_files):\n",
    "            words = src_file.read_text(encoding=\"utf-8\").split()\n",
    "            words = np.array(words[:len(words) - len(words) % max_length])\n",
    "            chunks = words.reshape(-1, max_length)\n",
    "            for example in chunks:\n",
    "                sentence = \" \".join(example)\n",
    "                self.examples += [sentence]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f9bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the Custom Dataset into train and test sets\n",
    "import random\n",
    "def create_train_test_datasets(tokenizer, max_length, fraction=1.0, test_size=0.1, valTest = False):\n",
    "    src_files = list(Path(PRIMUS_TXT_FILES).glob(\"**/*.txt\"))\n",
    "    src_files = src_files[:int(len(src_files) * fraction)]\n",
    "    \n",
    "    print(\"Number of source files:\")\n",
    "    print(len(src_files))\n",
    "    \n",
    "    sentences = CustomDataset(src_files, tokenizer, max_length=max_length).examples\n",
    "    random.shuffle(sentences)\n",
    "    \n",
    "    #Make a train, valid, and a test split\n",
    "    if valTest:\n",
    "        train_split = 1-(2*test_size)\n",
    "        train_sentences = sentences[:int(len(sentences)*train_split)]\n",
    "        test_sentences = sentences[int(len(sentences)*train_split):int(len(sentences)*(1-test_size))]\n",
    "        valid_sentences = sentences[int(len(sentences)*(1-test_size)):]\n",
    "        train_inputs = tokenizer(train_sentences, return_tensors='pt',max_length=max_length,truncation=True, padding='max_length')\n",
    "        test_inputs = tokenizer(test_sentences, return_tensors='pt',max_length=max_length,truncation=True, padding='max_length')\n",
    "        valid_inputs = tokenizer(valid_sentences, return_tensors='pt',max_length=max_length,truncation=True, padding='max_length')\n",
    "        train_inputs['labels'] = train_inputs.input_ids.detach().clone()\n",
    "        test_inputs['labels'] = test_inputs.input_ids.detach().clone()\n",
    "        valid_inputs['labels'] = valid_inputs.input_ids.detach().clone()\n",
    "        train_dataset = datasets.Dataset.from_dict(train_inputs)\n",
    "        test_dataset = datasets.Dataset.from_dict(test_inputs)\n",
    "        valid_dataset = datasets.Dataset.from_dict(valid_inputs)\n",
    "        \n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "    \n",
    "    #Only make a train/valid split\n",
    "    train_split = 1-test_size\n",
    "    train_sentences = sentences[:int(len(sentences)*train_split)]\n",
    "    test_sentences = sentences[int(len(sentences)*train_split):]\n",
    "    train_inputs = tokenizer(train_sentences, return_tensors='pt',max_length=max_length,truncation=True, padding='max_length')\n",
    "    test_inputs = tokenizer(test_sentences, return_tensors='pt',max_length=max_length,truncation=True, padding='max_length')\n",
    "    train_inputs['labels'] = train_inputs.input_ids.detach().clone()\n",
    "    test_inputs['labels'] = test_inputs.input_ids.detach().clone()\n",
    "    train_dataset = datasets.Dataset.from_dict(train_inputs)\n",
    "    test_dataset = datasets.Dataset.from_dict(test_inputs)\n",
    "    \n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c5c4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source files:\n",
      "1276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 1276/1276 [00:02<00:00, 550.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset = create_train_test_datasets(tokenizer, MAX_LEN, fraction=1.0, test_size=0.1, valTest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab34d76",
   "metadata": {},
   "source": [
    "# Huggingface Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c75798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT training code basically copied from the Huggingface Esperanto Tutorial from here on out\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=MASKING_PROPORTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8510482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc77d142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=100,\n",
       "evaluation_strategy=IntervalStrategy.STEPS,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=Extracted_Repns/MDE_M/model/runs/May04_05-57-23_mirlab6,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=100,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=2000,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "output_dir=Extracted_Repns/MDE_M/model,\n",
       "overwrite_output_dir=True,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=128,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['wandb'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=Extracted_Repns/MDE_M/model,\n",
       "save_on_each_node=False,\n",
       "save_steps=100,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=1,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")\n",
    "trainer.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0efaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 34533\n",
      "  Num Epochs = 2000\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 270000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mconati/StyleTransferClean/wandb/run-20220504_055727-1up6qz7g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mconati/huggingface/runs/1up6qz7g\" target=\"_blank\">Extracted_Repns/MDE_M/model</a></strong> to <a href=\"https://wandb.ai/mconati/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type dict that is 1310824 bytes\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3313' max='270000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3313/270000 1:58:14 < 158:44:30, 0.47 it/s, Epoch 24.53/2000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.297200</td>\n",
       "      <td>4.606528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.534900</td>\n",
       "      <td>4.502018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>4.431135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.431400</td>\n",
       "      <td>4.434926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.406600</td>\n",
       "      <td>4.400653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.412100</td>\n",
       "      <td>4.397254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.392500</td>\n",
       "      <td>4.381429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.364600</td>\n",
       "      <td>4.382532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.374800</td>\n",
       "      <td>4.393111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.353200</td>\n",
       "      <td>4.371119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.357400</td>\n",
       "      <td>4.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.355400</td>\n",
       "      <td>4.370533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.328900</td>\n",
       "      <td>4.312495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.284200</td>\n",
       "      <td>4.252133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.249000</td>\n",
       "      <td>4.221661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.211200</td>\n",
       "      <td>4.188733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.199200</td>\n",
       "      <td>4.164088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.163400</td>\n",
       "      <td>4.158201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.159000</td>\n",
       "      <td>4.146817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.136600</td>\n",
       "      <td>4.128018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.130100</td>\n",
       "      <td>4.104740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.113400</td>\n",
       "      <td>4.090681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.094600</td>\n",
       "      <td>4.092463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.090700</td>\n",
       "      <td>4.063171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.065600</td>\n",
       "      <td>4.072567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.059100</td>\n",
       "      <td>4.055216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.056700</td>\n",
       "      <td>4.063690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>4.043400</td>\n",
       "      <td>4.044471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.043800</td>\n",
       "      <td>4.023509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.018500</td>\n",
       "      <td>4.022188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>4.015300</td>\n",
       "      <td>4.012825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.007400</td>\n",
       "      <td>4.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.982100</td>\n",
       "      <td>3.966677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-100\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-100/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-200] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-200\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-200/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-100] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-300\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-300/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-200] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-400\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-400/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-400/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-500\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-500/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-400] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-600\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-600/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-500] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-700\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-700/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-600] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-800\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-800/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-800/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-900\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-900/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-800] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1000\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1000/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-700] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-900] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1100\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1100/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1000] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1200\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1200/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1200/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1300\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1300/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1100] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1200] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1400\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1400/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1300] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1500\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1500/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1400] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1600\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1600/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1500] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1700\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1700/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1700/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1600] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1800\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1800/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1800/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1700] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-1900\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-1900/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-1900/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1800] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2000\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2000/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-1900] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2100\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2100/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2100/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2000] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2200\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2200/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2200/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2100] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2300\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2300/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2300/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2400\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2400/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2400/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2300] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2500\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2500/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2500/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2600\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2600/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2600/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2400] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2500] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2700\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2700/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2700/pytorch_model.bin\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2800\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2800/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2800/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2600] due to args.save_total_limit\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2700] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-2900\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-2900/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-2900/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2800] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-3000\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-3000/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-2900] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-3100\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-3100/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-3100/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-3000] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-3200\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-3200/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-3200/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-3100] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4317\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to Extracted_Repns/MDE_M/model/checkpoint-3300\n",
      "Configuration saved in Extracted_Repns/MDE_M/model/checkpoint-3300/config.json\n",
      "Model weights saved in Extracted_Repns/MDE_M/model/checkpoint-3300/pytorch_model.bin\n",
      "Deleting older checkpoint [Extracted_Repns/MDE_M/model/checkpoint-3200] due to args.save_total_limit\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b30fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_SAVEDIR = Path(MDEDir + '/best')\n",
    "trainer.save_model(BEST_MODEL_SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2e3b0",
   "metadata": {},
   "source": [
    "# Testing the model and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b327053",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = True\n",
    "mpath = MDEDir + '/best/model/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f1d4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 107584877\n"
     ]
    }
   ],
   "source": [
    "if loadModel:\n",
    "    #Load a pretrained BERT model\n",
    "    model = BertForMaskedLM(config=config)\n",
    "    print('Num parameters:', model.num_parameters())\n",
    "\n",
    "\n",
    "    #Create a custom embedding class\n",
    "    temp = CustomBertEmbeddings(config)\n",
    "    #Replace the model's embedding layer\n",
    "    model.bert.embeddings = temp\n",
    "\n",
    "    temp = CustomBertLMPredictionHead(config)\n",
    "    model.cls.predictions = temp\n",
    "\n",
    "    mdict = torch.load(mpath)\n",
    "\n",
    "    model.load_state_dict(mdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fff81",
   "metadata": {},
   "source": [
    "### The following functions convert samples in the test set to sequences, and provide funtionality for getting model outputs on those samples. They also allow for quantifying accuracy of the model predictions(how often the model predicts exactly right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "453d0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts a dataset sample to a string that the model can perform inference on\n",
    "#Randomly adds masking tokens with the given probability\n",
    "\n",
    "def samp2seq(n, dataset, tokenizer, start = 0, stop = -1, maskProb = 0):\n",
    "    samp = dataset[n][\"input_ids\"][start:stop]\n",
    "\n",
    "    a = np.array(samp)\n",
    "    decoder = {value:key for key, value in tokenizer.vocab.items()}\n",
    "    b = [decoder[x] for x in a]\n",
    "    seq = \"\"\n",
    "    for idx, x in enumerate(b):\n",
    "        rand = random.random()\n",
    "        if rand < maskProb and x!='[CLS]' and x!='[SEP]':\n",
    "            x = \"[MASK]\"\n",
    "        if idx == 0:\n",
    "            seq = seq + str(x)\n",
    "        else:\n",
    "            seq = seq + \" \" + str(x)\n",
    "        if idx == len(b)-1:\n",
    "            seq = seq + \" \" + \"[SEP]\"\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94a37d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts a string to tokens with the given tokenizer, and finds model predictions on it\n",
    "\n",
    "def modelPred(seq, model, tokenizer):\n",
    "    device = model.device\n",
    "    tokenized = torch.Tensor(tokenizer(seq)[\"input_ids\"]).to(device)\n",
    "    tokenized = tokenized[1:-1].view(1, -1) #get rid of extra CLS and SEP tokens\n",
    "    preds = model(tokenized)['logits']\n",
    "    pitches = torch.argmax(preds[0], axis=2).detach().cpu().numpy().squeeze()\n",
    "    octaves = torch.argmax(preds[1], axis=2).detach().cpu().numpy().squeeze()\n",
    "    handConfs = torch.argmax(preds[2], axis=2).detach().cpu().numpy().squeeze()\n",
    "    numTokens = len(pitches)\n",
    "    \n",
    "    seq = \"\"\n",
    "    for idx, x in enumerate(octaves):\n",
    "        octave = octaves[idx]\n",
    "        pitch = pitches[idx]\n",
    "        hand = handConfs[idx]\n",
    "        MDE = str(octave)+ \"s\" + str(pitch)+ \"s\" + str(hand)\n",
    "        if idx>0 and idx<numTokens-2:\n",
    "            seq = seq + MDE + \" \"\n",
    "        if idx ==numTokens-2:\n",
    "            seq = seq + MDE\n",
    "    \n",
    "    return seq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69a5a0",
   "metadata": {},
   "source": [
    "Generate test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86f46c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] 2s10s1 5s0s1 4s3s32 5s0s1 3s0s144 4s8s1 5s0s1 4s3s20 [UNK] 4s8s3 4s4s1 3s8s1 2s8s1 2s10s1 4s5s20 5s0s1 4s9s23 2s8s1 [UNK] 5s0s1 4s6s40 3s3s1 2s3s60 4s6s23 3s8s600 2s8s36 5s0s1 4s6s1 5s0s1 4s2s1 3s2s8 5s0s1 3s3s74 4s6s1 4s3s26 3s10s1 4s8s1 3s8s1 4s5s1 3s5s1 4s3s444 3s3s30 5s0s1 4s8s1 4s6s1 3s5s20 4s6s40 5s0s1 [SEP]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = samp2seq(0, test_dataset, tokenizer, 0, 50)\n",
    "masked = samp2seq(0, test_dataset, tokenizer, 0, 50, maskProb=0.15)\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed1674bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] 2s10s1 5s0s1 4s3s32 5s0s1 3s0s144 4s8s1 5s0s1 4s3s20 [UNK] 4s8s3 4s4s1 3s8s1 2s8s1 2s10s1 4s5s20 5s0s1 4s9s23 [MASK] [UNK] 5s0s1 4s6s40 3s3s1 2s3s60 [MASK] 3s8s600 [MASK] [MASK] 4s6s1 5s0s1 4s2s1 3s2s8 [MASK] 3s3s74 4s6s1 4s3s26 3s10s1 4s8s1 [MASK] 4s5s1 3s5s1 4s3s444 3s3s30 5s0s1 4s8s1 4s6s1 3s5s20 4s6s40 [MASK] [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82209060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3s10s1 2s10s1 5s0s1 4s3s32 5s0s1 3s0s144 4s8s1 5s0s1 4s3s20 2s10s1 4s8s3 4s4s1 3s8s1 2s8s1 2s10s1 4s5s20 5s0s1 4s9s23 2s8s1 2s6s1 5s0s1 4s6s40 3s3s1 2s3s60 4s6s23 4s6s1 2s8s36 5s0s1 4s6s1 5s0s1 4s2s1 3s2s8 5s0s1 3s3s74 4s6s1 4s3s26 3s10s1 4s8s1 3s8s1 4s5s1 3s5s1 4s3s1 3s3s30 5s0s1 4s8s1 4s6s1 3s5s20 4s6s40 5s0s1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = modelPred(original, model, tokenizer)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52579a",
   "metadata": {},
   "source": [
    "Evaluate accuracy on a prediction (What percentage of masked tokens are predicted exactly right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "388b1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAcc(original, masked, pred):\n",
    "    pred = '[CLS] ' + pred + ' [SEP]'\n",
    "    original = original.split(' ')\n",
    "    masked = masked.split(' ')\n",
    "    pred = pred.split(' ')\n",
    "    \n",
    "    total = 0\n",
    "    octave = 0\n",
    "    pitch = 0\n",
    "    hand = 0\n",
    "\n",
    "    for x, item in enumerate(masked):\n",
    "        if item == '[MASK]' and original[x] !='[PAD]':\n",
    "            actual = original[x].split('s')\n",
    "            predicted = pred[x].split('s')\n",
    "            total +=1\n",
    "            \n",
    "\n",
    "            try:\n",
    "                if actual[0]==predicted[0]:\n",
    "                    octave +=1\n",
    "                if actual[1]==predicted[1]:\n",
    "                    pitch +=1\n",
    "                if actual[2]==predicted[2]:\n",
    "                    hand +=1\n",
    "            except:\n",
    "                #This is an unk tooooken\n",
    "                \n",
    "    if total > 0:\n",
    "        octaveAcc = octave/total\n",
    "        pitchAcc = pitch/total\n",
    "        handAcc = hand/total\n",
    "    else:\n",
    "        #NO MASKING OCCURRED!\n",
    "        return -1, -1, -1\n",
    "                \n",
    "\n",
    "    \n",
    "    return octaveAcc, pitchAcc, handAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780167",
   "metadata": {},
   "source": [
    "Make predictions on samples from an entire dataset (test set recommended), and return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20f26099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOverWholeSet(maskProb, train_dataset, tokenizer, model):\n",
    "    o_total = 0\n",
    "    p_total = 0\n",
    "    h_total = 0\n",
    "    total = 0\n",
    "    for sampleNum, x in enumerate(tqdm.tqdm(range(len(train_dataset)))):\n",
    "        original = samp2seq(sampleNum, train_dataset, tokenizer)\n",
    "        masked = samp2seq(sampleNum, train_dataset, tokenizer, maskProb=maskProb)\n",
    "        pred = modelPred(masked, model, tokenizer)\n",
    "\n",
    "        o, p, h = evaluateAcc(original, masked, pred)\n",
    "        if o == -1:\n",
    "            continue\n",
    "        o_total +=o\n",
    "        p_total +=p\n",
    "        h_total +=h\n",
    "        total +=1\n",
    "    if total != 0:\n",
    "        o_acc = o_total/total\n",
    "        p_acc = p_total/total\n",
    "        h_acc = h_total/total\n",
    "        return o_acc, p_acc, h_acc\n",
    "    return 1,1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e2b35",
   "metadata": {},
   "source": [
    "For fun, vary the masking proportion and plot accuracies over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a217677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def varyMaskAndPlot(maskProbs, test_dataset, tokenizer, model):\n",
    "    o_accs = []\n",
    "    p_accs = []\n",
    "    h_accs = []\n",
    "    for maskProb in maskProbs:\n",
    "        print(\"CHECKING MASKPROB: \" + str(maskProb))\n",
    "        o_acc, p_acc, h_acc = checkOverWholeSet(maskProb, test_dataset, tokenizer, model)\n",
    "        o_accs.append(o_acc)\n",
    "        p_accs.append(p_acc)\n",
    "        h_accs.append(h_acc)\n",
    "    \n",
    "    plt.plot(maskProbs, o_accs, 'g', label='Octave Accuracy')\n",
    "    plt.plot(maskProbs, p_accs, 'b', label='Pitch Accuracy')\n",
    "    plt.plot(maskProbs, h_accs, 'r', label='Hand Configuration Accuracy')\n",
    "\n",
    "\n",
    "    plt.title('Masking Percentage and Prediction Accuracy on Masked Tokens')\n",
    "\n",
    "    plt.xlabel('Masking Percentage')\n",
    "\n",
    "    plt.ylabel('Prediction Accuracy on Masked Tokens')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd140489",
   "metadata": {},
   "source": [
    "Check the accuracies on one sequence from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b8414e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '7', '1']\n",
      "['[UNK]']\n",
      "Octave Accuracy: 0.9285714285714286\n",
      "Pitch Accuracy: 0.7142857142857143\n",
      "Hand Configuration Accuracy: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "sampleNum = 1\n",
    "maskProb = 0.1\n",
    "\n",
    "original = samp2seq(sampleNum, test_dataset, tokenizer)\n",
    "masked = samp2seq(sampleNum, test_dataset, tokenizer, maskProb=maskProb)\n",
    "pred = modelPred(masked, model, tokenizer)\n",
    "\n",
    "accuracies = evaluateAcc(original, masked, pred)\n",
    "print(\"Octave Accuracy: {}\".format(accuracies[0]))\n",
    "print(\"Pitch Accuracy: {}\".format(accuracies[1]))\n",
    "print(\"Hand Configuration Accuracy: {}\".format(accuracies[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5e880",
   "metadata": {},
   "source": [
    "Check the average accuracy on all sequences in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12ea8f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                         | 11/4317 [00:01<08:38,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '1']\n",
      "['[UNK]']\n",
      "['6', '7', '1']\n",
      "['[UNK]']\n",
      "['3', '7', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                         | 14/4317 [00:01<08:11,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '4', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                         | 32/4317 [00:03<06:05, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '4', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                         | 58/4317 [00:05<06:00, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '11', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                                         | 66/4317 [00:06<06:22, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '2', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                                         | 74/4317 [00:06<06:06, 11.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '2', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                        | 94/4317 [00:08<05:57, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6', '2', '1']\n",
      "['[UNK]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                        | 97/4317 [00:08<06:22, 11.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_533854/1524672604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmaskProb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckOverWholeSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaskProb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_533854/2953069232.py\u001b[0m in \u001b[0;36mcheckOverWholeSet\u001b[0;34m(maskProb, train_dataset, tokenizer, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msampleNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamp2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamp2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaskProb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaskProb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelPred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maskProb = 0.1\n",
    "checkOverWholeSet(maskProb, test_dataset, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c1fd9",
   "metadata": {},
   "source": [
    "Check for many different masking probabilities and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskProbs = np.arange(20)/20\n",
    "varyMaskAndPlot(maskProbs, test_dataset, tokenizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
