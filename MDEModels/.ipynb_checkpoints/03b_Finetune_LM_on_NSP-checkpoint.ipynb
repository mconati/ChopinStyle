{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21010efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cebebf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from transformers import BertConfig, BertForMaskedLM, BertPreTrainedModel, BertModel, PreTrainedTokenizerFast, DataCollatorForLanguageModeling, BertPreTrainedModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from packaging import version\n",
    "import datasets\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from calculateMDE import calculateMDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1db16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Finetuning on NSP')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nums = [2.421766,\n",
    "1.573948,\n",
    "1.779308,\n",
    "1.348401,\n",
    "0.792499,\n",
    "0.803109,\n",
    "0.953968,\n",
    "0.897713,\n",
    "0.763546,\n",
    "0.691273,\n",
    "0.710557,\n",
    "0.715811,\n",
    "0.699343,\n",
    "0.69281,\n",
    "0.696763,\n",
    "0.718921,\n",
    "0.725978,\n",
    "0.711908,\n",
    "0.693623,\n",
    "0.694722,\n",
    "0.736117,\n",
    "0.773643,\n",
    "0.78013,\n",
    "0.756852,\n",
    "0.720938,\n",
    "0.694158,\n",
    "0.700915,\n",
    "0.723513,\n",
    "0.734817,\n",
    "0.7291,\n",
    "0.712036,\n",
    "0.709632,\n",
    "0.699967]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(nums)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Finetuning on NSP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c00a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDEDir = 'MDE43'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce118044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_file = open('/home/mconati/ttmp/styletransfer/NSP/measures.txt', \"r\")\n",
    "\n",
    "# list_of_lines = a_file.readlines()\n",
    "# for idx, line in enumerate(list_of_lines):\n",
    "#     if ',' not in line:\n",
    "#         newline = line.replace(' ', ',')\n",
    "#         list_of_lines[idx] = newline\n",
    "        \n",
    "# print(list_of_lines)\n",
    "\n",
    "# a_file = open('/home/mconati/ttmp/styletransfer/NSP/measures.txt', \"w\")\n",
    "\n",
    "# a_file.writelines(list_of_lines)\n",
    "\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e615eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/mconati/ttmp/styletransfer/NSP/measures.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aadf80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/mconati/ttmp/styletransfer/' + MDEDir +'/dict', 'rb') as handle:\n",
    "    hands = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e574adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_a = []\n",
    "# sentence_b = []\n",
    "# label = []\n",
    "\n",
    "# for idx, measure in enumerate(df['MEASURE']):\n",
    "#     sentence_a.append(calculateMDE(df['MEASURE'][idx], hands))\n",
    "#     sentence_b.append(calculateMDE(df['NEXT MEASURE'][idx], hands))\n",
    "#     label.append(0)\n",
    "    \n",
    "#     sentence_a.append(calculateMDE(df['MEASURE'][idx], hands))\n",
    "#     sentence_b.append(calculateMDE(df['RANDOM MEASURE'][idx], hands))\n",
    "#     label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa0ae6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MEASURE', 'NEXT MEASURE', 'RANDOM MEASURE'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MEASURE'][13]\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942ea8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     print(label[i])\n",
    "#     print(sentence_a[i] + '\\n---')\n",
    "#     print(sentence_b[i] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab179be",
   "metadata": {},
   "source": [
    "# Setting up MDE BERT\n",
    "The MDE representation consists of 3 things for every note onset in a piece:\n",
    "1. The pitch of the lowest note (a number 1-12 corresponding to A, A#, ....)\n",
    "2. The octave of the lowest note (a number 0-7 corresponding to which octave on the piano)\n",
    "3. The hand configuration (a number 0-136 that maps to a of hand configurations)\n",
    "\n",
    "To feed this information into BERT with standard huggingface components, I am using an intermediate, proxy tokenizer. At each step, the three elements are concatenated with a separation character. For example, a step with pitch 6, octave 2, and hand configuration 12 becomes 6s2s12. With this intermediate step, the sequence is dimension (1, max_seq_len) instead of (3, max_seq_len), so the default huggingface tokenizers, collators, and trainers can be used.\n",
    "\n",
    "Within the modified components, there are 3 separate embedding layers at the encoder and decoder(we effectively have three separate vocabularies). The first step in the encoder is to convert the MDE representation(ie. 6s2s12) back to a separate pitch, octave, hand representation so that it can be passed throgh three separate embedding layers.The outputs of the embedding layers can then be summed so that input to the attention layers has the normal shape (768). The decoder then has to have three separate linear layers to map the hidden state back to pitch, octave, hand.\n",
    "\n",
    "In order to make these changes in huggingface, we need to construct a custom encoder by modifying the BertEmbedding layer, and a custom decoder by modifying the BertLMPredictionHead. We also need to modify the BertForMaskedLM model itself so that the forward function expects three outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d1a14",
   "metadata": {},
   "source": [
    "# Unmodified huggingface functions\n",
    "\n",
    "Many things are used by the three huggingface components we are modifying(embeddings, prediction head, and maskedLM), but not all of them are importable from transformers. So, I had to copy/paste a lot of code from their source github so that the modified code can compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866ae813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "class GELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n",
    "    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +\n",
    "    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in nn.functional\n",
    "    Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_gelu_python: bool = False):\n",
    "        super().__init__()\n",
    "        if version.parse(torch.__version__) < version.parse(\"1.4\") or use_gelu_python:\n",
    "            self.act = self._gelu_python\n",
    "        else:\n",
    "            self.act = nn.functional.gelu\n",
    "\n",
    "    def _gelu_python(self, input: Tensor) -> Tensor:\n",
    "        return input * 0.5 * (1.0 + torch.erf(input / math.sqrt(2.0)))\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self.act(input)\n",
    "\n",
    "ACT2FN = {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"gelu\": GELUActivation(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "}\n",
    "\n",
    "class ModelOutput(OrderedDict):\n",
    "    \"\"\"\n",
    "    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n",
    "    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n",
    "    python dictionary.\n",
    "    <Tip warning={true}>\n",
    "    You can't unpack a `ModelOutput` directly. Use the [`~utils.ModelOutput.to_tuple`] method to convert it to a tuple\n",
    "    before.\n",
    "    </Tip>\n",
    "    \"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        class_fields = fields(self)\n",
    "\n",
    "        # Safety and consistency checks\n",
    "        if not len(class_fields):\n",
    "            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n",
    "        if not all(field.default is None for field in class_fields[1:]):\n",
    "            raise ValueError(f\"{self.__class__.__name__} should not have more than one required field.\")\n",
    "\n",
    "        first_field = getattr(self, class_fields[0].name)\n",
    "        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n",
    "\n",
    "        if other_fields_are_none and not is_tensor(first_field):\n",
    "            if isinstance(first_field, dict):\n",
    "                iterator = first_field.items()\n",
    "                first_field_iterator = True\n",
    "            else:\n",
    "                try:\n",
    "                    iterator = iter(first_field)\n",
    "                    first_field_iterator = True\n",
    "                except TypeError:\n",
    "                    first_field_iterator = False\n",
    "\n",
    "            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n",
    "            # set the associated fields\n",
    "            if first_field_iterator:\n",
    "                for element in iterator:\n",
    "                    if (\n",
    "                        not isinstance(element, (list, tuple))\n",
    "                        or not len(element) == 2\n",
    "                        or not isinstance(element[0], str)\n",
    "                    ):\n",
    "                        break\n",
    "                    setattr(self, element[0], element[1])\n",
    "                    if element[1] is not None:\n",
    "                        self[element[0]] = element[1]\n",
    "            elif first_field is not None:\n",
    "                self[class_fields[0].name] = first_field\n",
    "        else:\n",
    "            for field in class_fields:\n",
    "                v = getattr(self, field.name)\n",
    "                if v is not None:\n",
    "                    self[field.name] = v\n",
    "\n",
    "    def __delitem__(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def setdefault(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def pop(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if isinstance(k, str):\n",
    "            inner_dict = {k: v for (k, v) in self.items()}\n",
    "            return inner_dict[k]\n",
    "        else:\n",
    "            return self.to_tuple()[k]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name in self.keys() and value is not None:\n",
    "            # Don't call self.__setitem__ to avoid recursion errors\n",
    "            super().__setitem__(name, value)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        # Will raise a KeyException if needed\n",
    "        super().__setitem__(key, value)\n",
    "        # Don't call self.__setattr__ to avoid recursion errors\n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        \"\"\"\n",
    "        Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
    "        \"\"\"\n",
    "        return tuple(self[k] for k in self.keys())\n",
    "\n",
    "class BertForPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output type of [`BertForPreTraining`].\n",
    "    Args:\n",
    "        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        p1, p2, p3 = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return p1, p2, p3, seq_relationship_score\n",
    "        return p1, p2, p3\n",
    "    \n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        \n",
    "        #Three separate linear layers for converting hidden state to outputs\n",
    "        self.decode_pitch = nn.Linear(config.hidden_size, config.numPitches, bias=False)\n",
    "        self.decode_octave = nn.Linear(config.hidden_size, config.numOctaves, bias=False)\n",
    "        self.decode_hand = nn.Linear(config.hidden_size, config.numConfigs, bias=False)\n",
    "        \n",
    "        self.bias_p = nn.Parameter(torch.zeros(config.numPitches))\n",
    "        self.bias_o = nn.Parameter(torch.zeros(config.numOctaves))\n",
    "        self.bias_h = nn.Parameter(torch.zeros(config.numConfigs))\n",
    "\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decode_pitch.bias = self.bias_p\n",
    "        self.decode_octave.bias = self.bias_o\n",
    "        self.decode_hand.bias = self.bias_h\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        #Pass the hidden state through the three decoder layers\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        pitch = self.decode_pitch(hidden_states)\n",
    "        octave = self.decode_octave(hidden_states)\n",
    "        hand = self.decode_hand(hidden_states)\n",
    "        return pitch, octave, hand\n",
    "        \n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971ba06",
   "metadata": {},
   "source": [
    "# Modified huggingface code\n",
    "\n",
    "Here are the three elements that I modified from Huggingface. Modified elements are commented\n",
    "\n",
    "Modified MaskedLM, the forward function is changed to calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f60215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        self.decoder = {value:key for key, value in config.decoder.items()}\n",
    "        self.maskToken = config.decoder['[MASK]']\n",
    "        self.unkToken = config.decoder['[UNK]']\n",
    "        self.sepToken = config.decoder['[SEP]']\n",
    "        self.padToken = config.decoder['[PAD]']\n",
    "        self.clsToken = config.decoder['[CLS]']\n",
    "        self.specialTokens = [self.maskToken, self.unkToken, self.sepToken, self.padToken, self.clsToken]\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from transformers import BertTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "        >>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        pitch_p, octave_p, hand_p, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        \n",
    "\n",
    "        loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            \n",
    "        total_loss = 0# masked_lm_loss\n",
    "        if next_sentence_label is not None:\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = next_sentence_loss# + masked_lm_loss\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=[pitch_p, octave_p, hand_p],\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342322",
   "metadata": {},
   "source": [
    "### Custom encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ddd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from octave, pitch, hand configuration, and position.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #Get the mapping from token to encoded representation\n",
    "        self.decoder = {value:key for key, value in config.decoder.items()}\n",
    "        \n",
    "        #Aggregate special tokens\n",
    "        self.maskToken = config.decoder['[MASK]']\n",
    "        self.unkToken = config.decoder['[UNK]']\n",
    "        self.sepToken = config.decoder['[SEP]']\n",
    "        self.padToken = config.decoder['[PAD]']\n",
    "        self.clsToken = config.decoder['[CLS]']\n",
    "        self.specialTokens = [self.maskToken, self.unkToken, self.sepToken, self.padToken, self.clsToken]\n",
    "        \n",
    "        #Declare embedding layers\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.pitch_embeddings = nn.Embedding(config.numPitches, config.hidden_size)\n",
    "        self.handConfig_embeddings = nn.Embedding(config.numConfigs, config.hidden_size)\n",
    "        self.octave_embeddings = nn.Embedding(config.numOctaves, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        #Custom code to use 3 embedding layers\n",
    "        #Convert the tokenized MDE representation ie 9s2s55 to pitch=9 octave=2 hand=55 for all tokens in the batch\n",
    "        octaves = []\n",
    "        pitches = []\n",
    "        handConfs = []\n",
    "        #Iterate through the batch\n",
    "        for x in input_ids:\n",
    "            #For each sequence, make a list to store the octave, pitch, and handConf ids\n",
    "            octave = []\n",
    "            pitch = []\n",
    "            handConf = []\n",
    "            #Iterate through the sequence\n",
    "            for y in x:\n",
    "                #If the token is not a special token, decode into the octave_pitch_handConf representation\n",
    "                if y.item() not in self.specialTokens:\n",
    "                    #Split on s\n",
    "                    try:\n",
    "                        code = [int(x) for x in self.decoder[y.item()].split('s')]\n",
    "                    except:\n",
    "                        code = [x for x in self.decoder[y.item()].split('s')]\n",
    "                        print(code)\n",
    "                    #Add each element to the correct list\n",
    "                    octave.append(code[0])\n",
    "                    pitch.append(code[1])\n",
    "                    handConf.append(code[2])\n",
    "                else:\n",
    "                    #Otherwise, make a representation from the special token. ie: a cls token(1) becomes 1_1_1\n",
    "                    octave.append(y.item())\n",
    "                    pitch.append(y.item())\n",
    "                    handConf.append(y.item())\n",
    "            #Aggregate the samples in the batch\n",
    "            octaves.append(octave)\n",
    "            pitches.append(pitch)\n",
    "            handConfs.append(handConf)\n",
    "            \n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        \n",
    "        #Convert the lists to tensors and put them on the gpu\n",
    "        octTensor = torch.LongTensor(octaves).to(device)\n",
    "        pitchTensor = torch.LongTensor(pitches).to(device)\n",
    "        handConfTensor = torch.LongTensor(handConfs).to(device)\n",
    "        \n",
    "        #Sum the three embeddings\n",
    "        input_embeds = self.handConfig_embeddings(handConfTensor)\\\n",
    "                       +self.octave_embeddings(octTensor)\\\n",
    "                       +self.pitch_embeddings(pitchTensor)\n",
    "        embeddings = input_embeds\n",
    "\n",
    "        #Standard BertEmbeddings code\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531c633",
   "metadata": {},
   "source": [
    "### Custom Configuration\n",
    "\n",
    "In order for the encoder and MaskedLM to access the dictionary between MDE representation and tokens, we need to pass that in the model's config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72855cd",
   "metadata": {},
   "source": [
    "# THIS MUST BE CHANGED FOR DIFFERENT DATASETS. THE numCONFIGS COMES FROM CREATING THE HANDCONIG DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94d4a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertConfig(BertConfig):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #The decoder holds the conversion back to the coded representation for the customEmbeddings layer\n",
    "        self.decoder = kwargs.get('decoder')\n",
    "        self.numOctaves = 9\n",
    "        #110 for Chopin43\n",
    "        #720 for Maestro\n",
    "        #136 for ChopinAndHannds\n",
    "        self.numConfigs = 110\n",
    "        self.numPitches = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5f7a8",
   "metadata": {},
   "source": [
    "# (Mostly) standard huggingface code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b724a5a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmconati\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "#1dd35d404a289e1e49f18069e4fe0a51d28d52c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12121ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "186a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tokenizer\n",
    "TOKENIZER_SAVEDIR = Path('/home/mconati/ttmp/styletransfer/' + MDEDir + '/tokenizer')\n",
    "LM_MODEL_SAVEDIR = Path('/home/mconati/ttmp/styletransfer/' + MDEDir + '/model')\n",
    "Path(LM_MODEL_SAVEDIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59931978",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMUS_TXT_FILES = Path('/home/mconati/ttmp/styletransfer/' + MDEDir + '/text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2343163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7f5577d1cc50>, <torch.cuda.device at 0x7f5577d1cf90>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n"
     ]
    }
   ],
   "source": [
    "print('Cuda available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12795a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "MAX_LEN = 128\n",
    "MASKING_PROPORTION = 0.15\n",
    "TXT_LOCATION = Path('/home/mconati/ttmp/styletransfer/' + MDEDir + '/full')\n",
    "files = [str(TXT_LOCATION / path) for path in os.listdir(TXT_LOCATION)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb0eb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def makeVocab(MDEDir, threshold = 5):\n",
    "    tpath = '/home/mconati/ttmp/styletransfer/' + MDEDir + '/full/' + \"MDE.txt\"\n",
    "    my_file = open(tpath, \"r\")\n",
    "\n",
    "    # reading the file\n",
    "    data = my_file.read()\n",
    "\n",
    "    # replacing end of line('/n') with ' ' and\n",
    "    # splitting the text it further when '.' is seen.\n",
    "    data = np.array(data.replace('\\n', '').split(\" \"))[:-1]\n",
    "    print(data)\n",
    "\n",
    "    u,counts = np.unique(data, return_counts=True)\n",
    "    pairs = sorted(zip(counts,u), reverse=True)\n",
    "    counts, u = zip(*pairs)\n",
    "    \n",
    "    plt.plot(sorted(counts, reverse=True))\n",
    "    plt.title(\"Vocab\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Token number\")\n",
    "    \n",
    "    threshold = threshold\n",
    "    for idx, c in enumerate(counts):\n",
    "        if c < threshold:\n",
    "            break\n",
    "    vocab_size = idx\n",
    "\n",
    "    vocab = {}\n",
    "    vocab['[UNK]'] = 0\n",
    "    vocab['[CLS]'] = 1\n",
    "    vocab['[SEP]'] = 2\n",
    "    vocab['[PAD]'] = 3\n",
    "    vocab['[MASK]'] = 4\n",
    "    token_offset = 5\n",
    "    for i,qanon in enumerate(u):\n",
    "        if i > vocab_size:\n",
    "            break\n",
    "        vocab[qanon] = token_offset + i \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edc29fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer_utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eeef641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1s10s1' '3s5s2' '3s5s2' ... '2s8s1' '3s3s1' '3s11s1']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = makeVocab(MDEDir, 0)\n",
    "tokenizer = tokenizer_utils.BertTokenizer(vocab)\n",
    "\n",
    "tokenizer_dir = TOKENIZER_SAVEDIR\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "with open(f'{tokenizer_dir}/vocab.pkl','wb') as f:\n",
    "    pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7cb9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_a = []\n",
    "# sentence_b = []\n",
    "# label = []\n",
    "\n",
    "# for idx, measure in enumerate(df['MEASURE']):\n",
    "#     sentence_a.append(calculateMDE(df['MEASURE'][idx], hands))\n",
    "#     sentence_b.append(calculateMDE(df['NEXT MEASURE'][idx], hands))\n",
    "#     label.append(0)\n",
    "    \n",
    "#     sentence_a.append(calculateMDE(df['MEASURE'][idx], hands))\n",
    "#     sentence_b.append(calculateMDE(df['RANDOM MEASURE'][idx], hands))\n",
    "#     label.append(1)\n",
    "\n",
    "# inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=64, truncation=True, padding='max_length')\n",
    "# inputs['labels'] = torch.LongTensor([label]).T\n",
    "# with open ('inputs.pickle', 'wb') as handle:\n",
    "#     pickle.dump(inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d96ec067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  267,  312,  ...,    3,    3,    3],\n",
       "        [   1,  267,  312,  ...,    3,    3,    3],\n",
       "        [   1,  501,  547,  ...,    3,    3,    3],\n",
       "        ...,\n",
       "        [   1,   50,   29,  ...,    3,    3,    3],\n",
       "        [   1,   44, 1719,  ...,    3,    3,    3],\n",
       "        [   1,   44, 1719,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ...,\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('inputs.pickle', 'rb') as handle:\n",
    "    inputs = pickle.load(handle)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2794e1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36fb3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        returnee = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        returnee['next_sentence_label'] = returnee.pop('labels')\n",
    "        return returnee\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22bd83ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  1, 267, 312, 267, 312,   2, 267, 312, 267, 312,   2,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3,   3,   3,   3,   3,   3,   3]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'next_sentence_label': tensor([0])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NSPDataset(inputs)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07d35903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT training code basically copied from the Huggingface Esperanto Tutorial from here on out\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97806405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CustomBertConfig(\n",
    "    #The decoder holds the conversion back to the coded representation for the customEmbeddings layer\n",
    "    decoder = tokenizer.vocab,\n",
    "    vocab_size=len(vocab),\n",
    ")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4004d2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 88268677\n",
      "Num parameters: 86834821\n"
     ]
    }
   ],
   "source": [
    "#Create a standard BERT model\n",
    "model = BertForPreTraining(config=config)\n",
    "#device = model.device\n",
    "print('Num parameters:', model.num_parameters())\n",
    "\n",
    "\n",
    "#Create a custom embedding class\n",
    "temp = CustomBertEmbeddings(config).to(device)\n",
    "#Replace the model's embedding layer\n",
    "model.bert.embeddings = temp\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#As a sanity check, make sure that the custom embedding layers exist\n",
    "model.bert.embeddings.handConfig_embeddings.weight\n",
    "print('Num parameters:', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8510482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2500,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=100,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2217609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#decoder = {value:key for key, value in tokenizer.vocab.items()}# vals = []\n",
    "# for x in train_dataset:\n",
    "#     for y in x['input_ids']:\n",
    "#         if y>4:\n",
    "#             code = [int(x) for x in decoder[y].split('s')]\n",
    "#             if code[0] not in vals:\n",
    "#                 vals.append(code[0])\n",
    "            \n",
    "# vals\n",
    "# lowest = 8\n",
    "# octave = int(np.floor((lowest+21)/12)-2)+1\n",
    "# octave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc77d142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=1,\n",
       "evaluation_strategy=IntervalStrategy.STEPS,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=/home/mconati/ttmp/styletransfer/MDE43/model/runs/May02_18-03-43_mirlab6,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=1,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=2500,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "output_dir=/home/mconati/ttmp/styletransfer/MDE43/model,\n",
       "overwrite_output_dir=True,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=1,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['wandb'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=/home/mconati/ttmp/styletransfer/MDE43/model,\n",
       "save_on_each_node=False,\n",
       "save_steps=100,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=1,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "trainer.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c7aab",
   "metadata": {},
   "source": [
    "As a sanity check, look that the custom embeddings change by printing before and after training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "314ca529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1064, -0.7647,  0.0339,  ..., -0.1522,  0.5903, -0.0106],\n",
       "        [-0.1488, -0.8130,  2.1130,  ...,  2.0736,  0.2725, -0.1713],\n",
       "        [ 1.6889, -1.0059,  1.2381,  ..., -0.8915,  0.5835, -1.2289],\n",
       "        ...,\n",
       "        [ 0.1398, -0.2654,  0.3361,  ..., -0.8270,  0.5778, -0.7332],\n",
       "        [-1.1946,  1.0559, -1.0921,  ..., -0.9514,  0.3790,  1.4064],\n",
       "        [ 0.8194,  1.8508,  0.7484,  ..., -1.6917,  0.1525,  1.2034]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beforeTrainingHCEmbedding = model.bert.embeddings.handConfig_embeddings.weight\n",
    "beforeTrainingHCEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92fb7f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 88268677\n"
     ]
    }
   ],
   "source": [
    "loadModel = True\n",
    "BEST_MODEL_SAVEDIR = '/home/mconati/ttmp/styletransfer/' + MDEDir + '/MDE43best/model'\n",
    "\n",
    "if loadModel:\n",
    "    #Load a pretrained BERT model\n",
    "    model = BertForPreTraining(config=config)\n",
    "    print('Num parameters:', model.num_parameters())\n",
    "\n",
    "    #Create a custom embedding class\n",
    "    temp = CustomBertEmbeddings(config)\n",
    "    #Replace the model's embedding layer\n",
    "    model.bert.embeddings = temp\n",
    "\n",
    "    mdict = torch.load(BEST_MODEL_SAVEDIR + '/pytorch_model.bin')\n",
    "\n",
    "    model.load_state_dict(mdict, strict = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e0efaf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1612\n",
      "  Num Epochs = 2500\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2015000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/data0/mconati/styletransfer/BERT/wandb/run-20220502_180351-4spxstvd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mconati/huggingface/runs/4spxstvd\" target=\"_blank\">/home/mconati/ttmp/styletransfer/MDE43/model</a></strong> to <a href=\"https://wandb.ai/mconati/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/mconati/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 1.36 GiB already allocated; 5.44 MiB free; 1.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3310430/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m                         \u001b[0moptimizer_was_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_before\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mscale_after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moptimizer_was_run\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ttmp/anaconda3/envs/jukebox/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    346\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg_sq\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 1.36 GiB already allocated; 5.44 MiB free; 1.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e6fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "afterTrainingHCEmbedding = model.bert.embeddings.handConfig_embeddings.weight\n",
    "afterTrainingHCEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f70a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3186636/2307816136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBEST_MODEL_SAVEDIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/mconati/ttmp/styletransfer/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMDEDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/best/model/NSP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEST_MODEL_SAVEDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_SAVEDIR = Path('/home/mconati/ttmp/styletransfer/' + MDEDir + '/best/model/NSP')\n",
    "trainer.save_model(BEST_MODEL_SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2e3b0",
   "metadata": {},
   "source": [
    "# Code in progress from here on out. Testing to make sure the LM works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b327053",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = True\n",
    "mpath = '/home/mconati/ttmp/styletransfer/' + MDEDir + '/model/best/model/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = True\n",
    "mpath = '/home/mconati/ttmp/styletransfer/' + MDEDir + '/model/best/model/pytorch_model.bin'\n",
    "\n",
    "if loadModel:\n",
    "    #Load a pretrained BERT model\n",
    "    model = BertForPreTraining(config=config)\n",
    "    print('Num parameters:', model.num_parameters())\n",
    "\n",
    "\n",
    "    #Create a custom embedding class\n",
    "    temp = CustomBertEmbeddings(config)\n",
    "    #Replace the model's embedding layer\n",
    "    model.bert.embeddings = temp\n",
    "\n",
    "\n",
    "\n",
    "    mdict = torch.load(mpath)\n",
    "\n",
    "    model.load_state_dict(mdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def samp2seq(n, dataset, tokenizer, start = 0, stop = -1, maskProb = 0):\n",
    "    samp = dataset[n][\"input_ids\"][start:stop]\n",
    "\n",
    "    a = np.array(samp)\n",
    "    decoder = {value:key for key, value in tokenizer.vocab.items()}\n",
    "    b = [decoder[x] for x in a]\n",
    "    seq = \"\"\n",
    "    for idx, x in enumerate(b):\n",
    "        rand = random.random()\n",
    "        if rand < maskProb and x!='[CLS]' and x!='[SEP]':\n",
    "            x = \"[MASK]\"\n",
    "        if idx == 0:\n",
    "            seq = seq + str(x)\n",
    "        else:\n",
    "            seq = seq + \" \" + str(x)\n",
    "        if idx == len(b)-1:\n",
    "            seq = seq + \" \" + \"[SEP]\"\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a37d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelPred(seq, model, tokenizer):\n",
    "    device = model.device\n",
    "    tokenized = torch.Tensor(tokenizer(seq)[\"input_ids\"]).to(device)\n",
    "    tokenized = tokenized[1:-1].view(1, -1) #get rid of extra CLS and SEP tokens\n",
    "    preds = model(tokenized)['logits']\n",
    "    pitches = torch.argmax(preds[0], axis=2).detach().cpu().numpy().squeeze()\n",
    "    octaves = torch.argmax(preds[1], axis=2).detach().cpu().numpy().squeeze()\n",
    "    handConfs = torch.argmax(preds[2], axis=2).detach().cpu().numpy().squeeze()\n",
    "    numTokens = len(pitches)\n",
    "    \n",
    "    seq = \"\"\n",
    "    for idx, x in enumerate(octaves):\n",
    "        octave = octaves[idx]\n",
    "        pitch = pitches[idx]\n",
    "        hand = handConfs[idx]\n",
    "        MDE = str(octave)+ \"s\" + str(pitch)+ \"s\" + str(hand)\n",
    "        if idx>0 and idx<numTokens-2:\n",
    "            seq = seq + MDE + \" \"\n",
    "        if idx ==numTokens-2:\n",
    "            seq = seq + MDE\n",
    "    \n",
    "    return seq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69a5a0",
   "metadata": {},
   "source": [
    "Generate test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f46c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original = samp2seq(0, test_dataset, tokenizer, 0, 50)\n",
    "masked = samp2seq(0, test_dataset, tokenizer, 0, 50, maskProb=0.15)\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1674bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072918a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = modelPred(original, model, tokenizer)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAcc(original, masked, pred):\n",
    "    pred = '[CLS] ' + pred + ' [SEP]'\n",
    "    original = original.split(' ')\n",
    "    masked = masked.split(' ')\n",
    "    pred = pred.split(' ')\n",
    "    \n",
    "    total = 0\n",
    "    octave = 0\n",
    "    pitch = 0\n",
    "    hand = 0\n",
    "\n",
    "    for x, item in enumerate(masked):\n",
    "        if item == '[MASK]' and original[x] !='[PAD]':\n",
    "            actual = original[x].split('s')\n",
    "            predicted = pred[x].split('s')\n",
    "            total +=1\n",
    "            \n",
    "\n",
    "            try:\n",
    "                if actual[0]==predicted[0]:\n",
    "                    octave +=1\n",
    "                if actual[1]==predicted[1]:\n",
    "                    pitch +=1\n",
    "                if actual[2]==predicted[2]:\n",
    "                    hand +=1\n",
    "            except:\n",
    "                print(predicted)\n",
    "                print(actual)\n",
    "                \n",
    "    if total > 0:\n",
    "        octaveAcc = octave/total\n",
    "        pitchAcc = pitch/total\n",
    "        handAcc = hand/total\n",
    "    else:\n",
    "        #NO MASKING OCCURRED!\n",
    "        return -1, -1, -1\n",
    "                \n",
    "\n",
    "    \n",
    "    return octaveAcc, pitchAcc, handAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f26099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOverWholeSet(maskProb, train_dataset, tokenizer, model):\n",
    "    o_total = 0\n",
    "    p_total = 0\n",
    "    h_total = 0\n",
    "    total = 0\n",
    "    for sampleNum, x in enumerate(tqdm(range(len(train_dataset)))):\n",
    "        original = samp2seq(sampleNum, train_dataset, tokenizer)\n",
    "        masked = samp2seq(sampleNum, train_dataset, tokenizer, maskProb=maskProb)\n",
    "        pred = modelPred(masked, model, tokenizer)\n",
    "\n",
    "        o, p, h = evaluateAcc(original, masked, pred)\n",
    "        if o == -1:\n",
    "            continue\n",
    "        o_total +=o\n",
    "        p_total +=p\n",
    "        h_total +=h\n",
    "        total +=1\n",
    "    if total != 0:\n",
    "        o_acc = o_total/total\n",
    "        p_acc = p_total/total\n",
    "        h_acc = h_total/total\n",
    "        return o_acc, p_acc, h_acc\n",
    "    return 1,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a217677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def varyMaskAndPlot(maskProbs, test_dataset, tokenizer, model):\n",
    "    o_accs = []\n",
    "    p_accs = []\n",
    "    h_accs = []\n",
    "    for maskProb in maskProbs:\n",
    "        print(\"CHECKING MASKPROB: \" + str(maskProb))\n",
    "        o_acc, p_acc, h_acc = checkOverWholeSet(maskProb, test_dataset, tokenizer, model)\n",
    "        o_accs.append(o_acc)\n",
    "        p_accs.append(p_acc)\n",
    "        h_accs.append(h_acc)\n",
    "    \n",
    "    plt.plot(maskProbs, o_accs, 'g', label='Octave Accuracy')\n",
    "    plt.plot(maskProbs, p_accs, 'b', label='Pitch Accuracy')\n",
    "    plt.plot(maskProbs, h_accs, 'r', label='Hand Configuration Accuracy')\n",
    "\n",
    "\n",
    "    plt.title('Masking Percentage and Prediction Accuracy on Masked Tokens')\n",
    "\n",
    "    plt.xlabel('Masking Percentage')\n",
    "\n",
    "    plt.ylabel('Prediction Accuracy on Masked Tokens')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd140489",
   "metadata": {},
   "source": [
    "Check the accuracies on one sequence from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8414e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleNum = 11\n",
    "maskProb = 0.1\n",
    "\n",
    "original = samp2seq(sampleNum, train_dataset, tokenizer)\n",
    "masked = samp2seq(sampleNum, train_dataset, tokenizer, maskProb=maskProb)\n",
    "pred = modelPred(masked, model, tokenizer)\n",
    "\n",
    "accuracies = evaluateAcc(original, masked, pred)\n",
    "print(\"Octave Accuracy: {}\".format(accuracies[0]))\n",
    "print(\"Pitch Accuracy: {}\".format(accuracies[1]))\n",
    "print(\"Hand Configuration Accuracy: {}\".format(accuracies[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5e880",
   "metadata": {},
   "source": [
    "Check the average accuracy on all sequences in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea8f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maskProb = 0.1\n",
    "checkOverWholeSet(maskProb, train_dataset, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c1fd9",
   "metadata": {},
   "source": [
    "Check for many different masking probabilities and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskProbs = np.arange(20)/20\n",
    "varyMaskAndPlot(maskProbs, test_dataset, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c07a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
